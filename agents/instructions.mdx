---
title: Instructions (Prompts)
---

Under the hood an Agent will convert the `description` and `instructions` into a **system** prompt and send the input message as a **user** prompt.

<Note>
This is merely a formatting benefit, we DO NOT ALTER or abstract any information.
</Note>

## System Prompt

The `description` is added to the start of the system prompt and `instructions` are added as a numbered list inside  `<instructions></instructions>` tags. For example:

```python instructions.py
from phi.agent import Agent

agent = Agent(
    description="You are a famous short story writer asked to write for a magazine",
    instructions=["You are a pilot on a plane flying from Hawaii to Japan."],
    markdown=True,
    debug_mode=True,
)
agent.print_response("Tell me a 2 sentence horror story.")
```

Will translate to:

```js
DEBUG    ============== system ==============
DEBUG    You are a famous short story writer asked to write for a magazine
         YOU MUST FOLLOW THESE INSTRUCTIONS CAREFULLY.
         <instructions>
         1. You are a pilot on a plane flying from Hawaii to Japan.
         2. Use markdown to format your answers.
         </instructions>
DEBUG    ============== user ==============
DEBUG    Tell me a 2 sentence horror story.
DEBUG    Time to generate response: 1.5415s
DEBUG    Estimated completion tokens: 40
```

## User Prompt

In most cases, the user prompt is simply the input `message` sent to the Agent.

If a `knowledge_base` is provided and `add_references_to_prompt=True`, the user prompt is updated to include:

```python
user_prompt += f"""Use this information from the knowledge base if it helps:
<knowledge_base>
{references}
</knowledge_base>
"""
```

If `add_chat_history_to_prompt=True`, the user prompt is updated to include messages from the chat history. The number of messages can by set using `num_history_messages`:

```python
user_prompt += f"""Use the following chat history to reference past messages:
<chat_history>
{chat_history}
</chat_history>
"""
```

## Overriding Default Prompts

**You can completely override the default prompts using:**

<ResponseField name="system_prompt" type="str" default="None">
  Provide the system prompt as a string.
</ResponseField>
<ResponseField name="system_prompt_template" type="PromptTemplate" default="None">
  Provide the system prompt as a PromptTemplate
</ResponseField>
<ResponseField name="user_prompt" type="str" default="None">
  Provide the user prompt as a string. Note: this will ignore the message sent to the run function.
</ResponseField>
<ResponseField name="user_prompt_template" type="PromptTemplate" default="None">
  Provide the user prompt as a PromptTemplate.
</ResponseField>

Example:

```python
from phi.agent import Agent

agent = Agent(
    system_prompt="Share a 2 sentence story about",
    user_prompt="Love in the year 12000.",
    debug_mode=True,
)
agent.print_response()
```

## Customize the System Prompt

The system prompt can be customized using:

<ResponseField name="description" type="str" default="None">
  A description of the Agent that is added to top the system prompt.
</ResponseField>
<ResponseField name="instructions" type="List[str]" default="None">
  List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `output_model` etc.
</ResponseField>
<ResponseField name="extra_instructions" type="List[str]" default="None">
  List of extra_instructions added to the default system prompt. Use these when you want to add some extra instructions at the end of the default instructions.
</ResponseField>
<ResponseField name="add_to_system_prompt" type="str" default="None">
  Add a string to the end of the default system prompt.
</ResponseField>
<ResponseField name="add_knowledge_base_instructions" type="bool" default="True">
  If True, add instructions for using the knowledge base to the system prompt if knowledge base is provided
</ResponseField>
<ResponseField name="prevent_hallucinations" type="bool" default="False">
  If True, add instructions to return "I dont know" when the agent does not know the answer.
</ResponseField>
<ResponseField name="prevent_prompt_injection" type="bool" default="False">
  If True, add instructions to prevent prompt injection attacks.
</ResponseField>
<ResponseField name="limit_tool_access" type="bool" default="False">
  If True, add instructions for limiting tool access to the default system prompt if tools are provided
</ResponseField>
<ResponseField name="add_datetime_to_instructions" type="bool" default="False">
  If True, add the current datetime to the prompt to give the agent a sense of time. This allows for relative times like "tomorrow" to be used in the prompt
</ResponseField>
<ResponseField name="markdown" type="bool" default="False">
  Add an instruction to format the output using markdown.
</ResponseField>
<ResponseField name="output_model" type="Optional[Union[str, List, Type[BaseModel]]]" default="None">
  Provide an output model for the responses. Accepts a pydantic model, list of strings where each string is a key the LLM should return a value for or just a string.
</ResponseField>
<ResponseField name="parse_output" type="bool" default="True">
  If True, the output is converted into the output_model (pydantic model or json dict). Otherwise returned as is.
</ResponseField>

## Customize the User Prompt

The user prompt can be customized using:

<ResponseField name="add_references_to_prompt" type="bool" default="False">
  Enable RAG by adding references from the knowledge base to the prompt.
</ResponseField>
<ResponseField name="add_chat_history_to_prompt" type="bool" default="False">
  Adds the formatted chat history to the user prompt.
</ResponseField>
<ResponseField name="num_history_messages" type="int" default="6">
  Number of previous messages to add to the prompt or messages.
</ResponseField>