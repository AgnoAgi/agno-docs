---
title: Mistral
---

## Example

<CodeGroup>

```python agent.py
import os

from phi.agent import Agent, RunResponse  # noqa
from phi.model.mistral import MistralChat

mistral_api_key = os.getenv("MISTRAL_API_KEY")

agent = Agent(
    model=MistralChat(
        id="mistral-large-latest",
        api_key=mistral_api_key,
    ),
    markdown=True,
    debug_mode=True,
)

# Get the response in a variable
# run: RunResponse = agent.run("Share a 2 sentence horror story")
# print(run.content)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story")

```

</CodeGroup>

## Mistral Params

<ResponseField name="name" type="str" default="Mistral"></ResponseField>
<ResponseField
  name="id"
  type="str"
  default="mistral-large-latest"
></ResponseField>
<ResponseField name="temperature" type="float"></ResponseField>
<ResponseField name="max_tokens" type="int" default="1024"></ResponseField>
<ResponseField name="top_p" type="float"></ResponseField>
<ResponseField name="random_seed" type="int"></ResponseField>
<ResponseField name="safe_mode" type="bool"></ResponseField>
<ResponseField name="safe_prompt" type="bool"></ResponseField>
<ResponseField
  name="response_format"
  type="Union[Dict[str, Any], ChatCompletionResponseFormat]"
></ResponseField>

<ResponseField name="api_key" type="str"></ResponseField>
<ResponseField name="endpoint" type="str"></ResponseField>
<ResponseField name="max_retries" type="int"></ResponseField>
<ResponseField name="timeout" type="int"></ResponseField>
<ResponseField name="mistral_client" type="MistralClient"></ResponseField>

## LLM Params

`Mistral` is a subclass of the `Model` class and has access to the same params

<Snippet file="llm-base-reference.mdx" />
