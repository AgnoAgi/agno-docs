---
title: Groq
---

## Example

<CodeGroup>

```python assistant.py
from phi.assistant import Assistant
from phi.llm.groq import Groq

assistant = Assistant(
    llm=Groq(model="mixtral-8x7b-32768"),
    description="You help people with their health and fitness goals.",
    # debug_mode=True,
)
assistant.print_response("Share a quick healthy breakfast recipe.", markdown=True)
```

</CodeGroup>

## Groq Params

<ResponseField name="name" type="str" default="Groq"/>
<ResponseField name="model" type="str" default="mixtral-8x7b-32768"/>
<ResponseField name="frequency_penalty" type="float">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on
  their existing frequency in the text so far, decreasing the model's likelihood
  to repeat the same line verbatim.
</ResponseField>
<ResponseField name="logit_bias" type="Any">
  Modify the likelihood of specified tokens appearing in the completion. Accepts
  a json object that maps tokens (specified by their token ID in the tokenizer)
  to an associated bias value from -100 to 100.
</ResponseField>
<ResponseField name="logprobs" type="int"/>
<ResponseField name="max_tokens" type="int">
  The maximum number of tokens to generate in the chat completion.
</ResponseField>
<ResponseField name="presence_penalty" type="float">
  Number between -2.0 and 2.0. Positive values penalize new tokens based on
  whether they appear in the text so far, increasing the model's likelihood to
  talk about new topics.
</ResponseField>
<ResponseField name="response_format" type="Dict[str, Any]">
  An object specifying the format that the model must output.
  Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
</ResponseField>
<ResponseField name="seed" type="int">
  If specified, openai system will make a best effort to sample
  deterministically, such that repeated requests with the same `seed` and
  parameters should return the same result.
</ResponseField>
<ResponseField name="stop" type="[Union[str, List[str]]">
  Up to 4 sequences where the API will stop generating further tokens.
</ResponseField>
<ResponseField name="temperature" type="float">
  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
  make the output more random, while lower values like 0.2 will make it more
  focused and deterministic.
</ResponseField>
<ResponseField name="top_logprobs" type="int"/>
<ResponseField name="top_p" type="float">
  An alternative to sampling with temperature, called nucleus sampling, where
  the model considers the results of the tokens with top_p probability mass.
</ResponseField>
<ResponseField name="user" type="str">
  A unique identifier representing your end-user, which can help OpenAI to
  monitor and detect abuse.
</ResponseField>
<ResponseField name="extra_headers" type="Any"/>
<ResponseField name="extra_query" type="Any"/>
<ResponseField name="api_key" type="str"/>
<ResponseField name="organization" type="str"/>
<ResponseField name="base_url" type="str"/>
<ResponseField name="timeout" type="float"/>
<ResponseField name="max_retries" type="int"/>
<ResponseField name="default_headers" type="Any"/>
<ResponseField name="default_query" type="Any"/>
<ResponseField name="groq_client" type="GroqClient"/>

## LLM Params

`Groq` is a subclass of the `LLM` class and has access to the same params

<Snippet file="llm-base-reference.mdx" />