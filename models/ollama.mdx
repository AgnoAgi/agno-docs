---
title: Ollama
---

Run Large Language Models locally with Ollama

[Ollama](https://ollama.com) is a fantastic tool for running models locally. Install [ollama](https://ollama.com) and run a model using

<CodeGroup>

```bash run model
ollama run llama3.1
```

```bash serve
ollama serve
```

</CodeGroup>

After you have the local model running, use the `Ollama` model to access them

## Example

<CodeGroup>

```python agent.py
from agno.agent import Agent, RunResponse
from agno.models.ollama import Ollama

agent = Agent(
    model=Ollama(id="llama3.1"),
    markdown=True
)

# Print the response in the terminal
agent.print_response("Share a 2 sentence horror story.")
```

</CodeGroup>

## Params

<Snippet file="model-ollama-params.mdx" />

`Ollama` is a subclass of the [Model](/reference/model) class and has access to the same params.
